<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->

  <!-- Website verification -->
  
    <meta name="google-site-verification" content="">
  
  
  <!--
    Avoid warning on Google Chrome Error with Permissions-Policy header:
    Origin trial controlled feature not enabled: 'interest-cohort'.
    see https://stackoverflow.com/a/75119417
  -->
  <meta http-equiv="Permissions-Policy" content="interest-cohort=()">




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    Zhaolun Will Su
  
</title>
<meta name="author" content="Zhaolun Su">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="Zhaolun Will Su, Meta FAIR, LLM, Large Language Models, Vision Language Models, AI Research, Machine Learning, NLP, Reasoning, Alignment">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%AD&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://zhsu-private.github.io/">


  <!-- Dark Mode -->
  <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>
  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>










  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">home
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/news/">news
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">
      
        Zhaolun Will Su
      
    </h1>
    <p class="desc"></p>
  </header>

  <article>
    
      <div class="profile float-right">
        
          
          
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/img/working-480.webp 480w,/assets/img/working-800.webp 800w,/assets/img/working-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px)
      30vw, 95vw">
      
    
    <img src="/assets/img/working.jpg?74dcfbf3b75296e88c09678476d103a7" class="img-fluid z-depth-1
      rounded" width="100%" height="auto" alt="working.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </source></picture>

  
</figure>

        
        
      </div>
    

    <div class="clearfix">
<p>Hi, I’m Zhaolun (Will) Su, a researcher specializing in large language models, vision-language systems, and post-training alignment. My work focuses on building scalable pipelines that enable models to self-improve, generate high-quality synthetic data, and learn robust reasoning behaviors without relying on stronger teacher models.</p>

<p>I’ve led major cross-team research efforts at <a href="https://ai.meta.com/research/" rel="external nofollow noopener" target="_blank">Meta MRS AI</a>, driving improvements in LLM reasoning, VLM post-training, and self-alignment, and previously worked across autonomous driving and mapping organizations at AutoX, Google, Waymo. My research interests center on reinforcement learning for LLMs, self-training, synthetic data generation, and alignment methods that make large models more adaptive, reliable, and aligned with human preferences.</p>

<p>I’m passionate about bridging advanced research with practical, production-ready systems—designing models and pipelines that scale, generalize, and deliver measurable impact in real-world applications such as recommendation and agentic systems.</p>

<div style="margin-top: 2rem;">
  <p><strong>Contact:</strong> <a href="mailto:zhsu@umich.edu">zhsu@umich.edu</a> <em></em></p>
</div>

<hr>

<h2 style="text-align: center; margin-top: 3rem; margin-bottom: 2.5rem; font-size: 2rem; font-weight: 300; color: #2c3e50;">Publications &amp; Projects</h2>

<div style="background-color: #f7f8fa; padding: 0.9rem 1.5rem; margin-top: 2rem; margin-bottom: 1.2rem; border-left: 4px solid #6b7c8f; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.04); transition: all 0.3s ease;">
  <h3 style="margin: 0; color: #2c3e50; font-size: 1.2rem; font-weight: 600; letter-spacing: 0.2px;">Improving LLM Reasoning Ability</h3>
</div>

<p><strong>RESTRAIN: From Spurious Votes to Signals — Self-Driven RL with Self-Penalization</strong><br>
Zhaoning Yu†, <u>Will Su†</u>,, Leitian Tao, Haozhu Wang, Aashu Singh, Hanchao Yu, Jianyu Wang, Hongyang Gao, Weizhe Yuan, Jason Weston, Ping Yu‡, Jing Xu‡.  († Equal contribution)</p>

<p><em>2025</em> [<a href="https://arxiv.org/pdf/2510.02172" rel="external nofollow noopener" target="_blank">PDF</a>]</p>

<p>RESTRAIN (2025) proposes a label-free reinforcement-learning method that lets LLMs self-improve on reasoning tasks without any human-provided answers. Instead of trusting majority-vote pseudo-labels—which are often wrong on hard problems—RESTRAIN uses the full distribution of sampled answers, weighting each candidate answer by its vote frequency, and penalizes prompts where the model shows low self-consistency (i.e., no answer is reliable). It further downweights inherently uncertain prompts using a frozen reference model, preventing the RL loop from amplifying spurious confidence. Combined with GRPO-style policy optimization, this yields strong empirical gains: on multiple benchmarks, RESTRAIN outperforms prior unsupervised RL baselines and often matches or even exceeds gold-label supervised RL—despite using zero human labels. The result is a robust and scalable approach for self-training LLMs on reasoning tasks using only unlabeled data.</p>

<hr>

<p><strong>SVR-R1: Bootstrapping Multi-modal Reasoning with Self-verification in Reinforcement Learning</strong><br>
Mingyuan Wu, Jingcheng Yang, Shengyi Qian, Xudong Wang, Jize Jiang, Qifan Wang, Aashu Singh, Khoi Pham, Claire Liu, <u>Will Su</u>, Zhuokai Zhao, Klara Nahrstedt, Jianyu Wang, Hanchao Yu.<br>
<em>2025</em></p>

<p>Self-Verified Reasoner (SVR-R1), a novel reinforcement learning (RL) framework designed to enhance multimodal reasoning in vision-language models (VLMs). SVR-R1 leverages a model’s own self-verification as a learning signal: for each query, the model proposes an answer and then uses its own weights to verify the answer with a binary verdict (Yes/No). If the verdict is “No,” the model is prompted to rethink and regenerate its answer; if “Yes,” or after a set number of turns, the answer is finalized for reward computation. SVR-R1 is implemented using Group Relative Policy Optimization (GRPO) and an asynchronous multi-turn rollout framework, requiring no external supervision or auxiliary critics. Experiments on challenging vision–language reasoning benchmarks show that SVR-R1 significantly improves accuracy over strong GRPO baselines, with models gradually relying less on verification rounds as they internalize self-correction. This approach bridges the gap between inference-time self-refinement and RL training for VLMs, offering a simple yet effective method for bootstrapping multimodal reasoning and paving the way for future research in self-improving VLMs.</p>

<hr>

<div style="background-color: #f7f8fa; padding: 0.9rem 1.5rem; margin-top: 2rem; margin-bottom: 1.2rem; border-left: 4px solid #6b7c8f; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.04); transition: all 0.3s ease;">
  <h3 style="margin: 0; color: #2c3e50; font-size: 1.2rem; font-weight: 600; letter-spacing: 0.2px;">Autonomous Driving &amp; HD Map</h3>
</div>

<p><strong>Google Geo HD Map</strong><br>
Semantic Surfel Based Traffic Control Items Curation Designed and implemented semantic classification models for street view surfels, which are used to curate traffic control items including lane markers, flow-lines, lane connectivity, merges, and splits. Implemented pipelines to scale model deployment at Google scale, and metrics to monitor long-term model performance(Accuracy, Latency, etc). Tech: Vision Transformer, Multiple View Geometry, Map Reduce, etc</p>

<hr>

<p><strong>Waymo Perception</strong><br>
Multimodal FOD Perception - LiDAR/Camera/HD Map Designed, implemented, and deployed ML ML-based real-time lane surface model, that performs geometry mapping guided by onboard attention. The module simultaneously produces the elevation, norm and incline angle of lane surfaces over ROI. Built pipelines to supervise the model’s long-term performance as the fleet adapts to different operation domains. Improved model latency with optimized mathematical models including CNN, RANSAC, PCA and ICP matching.</p>

<hr>

<div style="background-color: #f7f8fa; padding: 0.9rem 1.5rem; margin-top: 2rem; margin-bottom: 1.2rem; border-left: 4px solid #6b7c8f; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.04); transition: all 0.3s ease;">
  <h3 style="margin: 0; color: #2c3e50; font-size: 1.2rem; font-weight: 600; letter-spacing: 0.2px;">Surgical Robotics</h3>
</div>

<p><strong>Integration of Autopatching with Automated Pipette and Cell Detection in Vitro</strong><br>
Qiuyu Wu, Ilya Kolb, Brendan M Callahan, <u>Zhaolun Su</u>, William Stoy, Suhasa B Kodandaramaiah, Rachael Neve, Hongkui Zeng, Edward S Boyden, Craig R Forest, Alexander A Chubykin.<br>
<em>2016</em> [<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=nzkurd4AAAAJ&amp;citation_for_view=nzkurd4AAAAJ:d1gkVwhDpl0C" rel="external nofollow noopener" target="_blank">PDF</a>]</p>

<div style="text-align: center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/V9QZI-y7LVo?si=FXG83RKvRYGm0Fh-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<p>We introduces an automated, computer-vision-guided patch-clamp system that integrates real-time pipette tip tracking with robust, label-free cell detection in acute brain slices, enabling a fully automated approach to in-vitro whole-cell electrophysiology. The system uses image-based algorithms to locate and continuously track the pipette in 3-D, identify candidate neurons from DIC/IR-DIC imagery, and dynamically guide the pipette toward the cell membrane with closed-loop visual feedback. By automating critical manual steps—pipette approach, membrane contact detection, seal formation, and break-in—the method achieves reliable whole-cell recordings without human intervention. This computer-vision-driven pipeline demonstrates that high-quality patch-clamp experiments can be achieved through precise image-based control, significantly improving throughput, standardization, and reproducibility relative to manual electrophysiology.</p>

<hr>

<p><strong>US Patent: Systems and methods for automated image-guided patch-clamp electrophysiology in vitro</strong><br>
Alexander A Chubykin, <u>Zhaolun Su</u>, Qiuyu Wu 
<em>2016</em> [<a href="https://patents.google.com/patent/US10324080B2/en" rel="external nofollow noopener" target="_blank">Patent</a>]</p>

<p>This patent (US 10324080B2) introduces a fully automated, image-guided patch-clamp system that uses real-time computer vision and robotic micromanipulation to perform whole-cell electrophysiology in vitro without human intervention. The invention automatically detects cells from label-free microscope images, tracks the pipette tip in 3-D, and precisely guides the pipette to each target neuron while controlling pressure and seal formation through a closed-loop feedback process. By automating all major steps of the traditional manual patch-clamp workflow, the system delivers high-quality, reproducible recordings at significantly higher throughput, lowering the skill barrier and enabling large-scale, systematic electrophysiological studies.</p>

<hr>

<div style="text-align: center; font-size: 2.5rem; margin-top: 3rem; margin-bottom: 2rem;">
  <a href="https://www.linkedin.com/in/zhaolun-will-su/" title="LinkedIn" target="_blank" style="margin: 0 1rem;" rel="external nofollow noopener"><i class="fab fa-linkedin"></i></a>
   <a href="https://scholar.google.com/citations?user=nzkurd4AAAAJ" title="Google Scholar" target="_blank" style="margin: 0 1rem;" rel="external nofollow noopener"><i class="ai ai-google-scholar"></i></a>
  <a href="https://github.com/zhsu-private" title="GitHub" target="_blank" style="margin: 0 1rem;" rel="external nofollow noopener"><i class="fab fa-github"></i></a>
</div>
</div>

    <!-- News -->
    

    <!-- Latest posts -->
    

    <!-- Selected papers -->
    

    <!-- Social -->
    

    
  </article>
</div>

      
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  © Copyright 2025
  Zhaolun
  
  Su. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

  
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script>
<script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->

  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  









  <!-- Scrolling Progress Bar -->
  <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/assets/js/search-data.js"></script>
  <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>




  </body>
</html>
