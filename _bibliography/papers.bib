% 将这个文件保存为 _bibliography/papers.bib

@article{tao2025moving,
  title={Moving Beyond Binary Verifiable Reward},
  author={Tao, Leitian and Kulikov, Ilia and Weston, Jason and Yu, Ping},
  year={2025},
  selected={true},
  preview={reasoning.png},
  abstract={Current research on reasoning tasks mainly focuses on verifiable rewards. We studied whether using verifiable answers for GRPO can help with hard-to-verify reasoning tasks, and explored whether including hard-to-verify training data is necessary. We proposed combining reward model signals with rule-based signals for model training.}
}

@article{yu2024distilling,
  title={Distilling System 2 into System 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  year={2024},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={distill.png},
  abstract={Investigate self-supervised methods to 'compile' (distill) higher quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1.}
}

@article{yu2025cot,
  title={CoT-Self-Instruct: Building High-Quality Synthetic Prompts for Reasoning and Non-Reasoning Tasks},
  author={Yu, Ping and Lanchantin, Jack and Wang, Tianlu and others},
  year={2025},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={cot_instruct.png},
  abstract={Proposed CoT-Self-Instruct, a synthetic data generation method that uses Chain-of-Thought reasoning and automatic filtering to create high-quality training data. Achieves state-of-the-art results on verifiable reasoning benchmarks.}
}

@inproceedings{yu2025rip,
  title={RIP: Better Models by Survival of the Fittest Prompts},
  author={Yu, Ping and Yuan, Weizhe and others},
  booktitle={ICML},
  year={2025},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={rip.png},
  abstract={Proposed Rejecting Instruction Preferences (RIP), a data filtering method that evaluates training prompts via rejected response quality and reward gap. Achieved +9.4% AlpacaEval2, +8.7% Arena-Hard, +9.9% WildBench gains.}
}

@inproceedings{li2024self,
  title={Self-Alignment with Instruction Backtranslation},
  author={Li, Xian and Yu, Ping and Zhou, Chunting and others},
  booktitle={ICLR (Oral)},
  year={2024},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={backtranslation.png},
  abstract={Developed instruction backtranslation, a scalable method to train instruction-following LLMs by automatically generating and curating instruction-response pairs from web text.}
}

@article{yu2025restrain,
  title={ReStrain: Reinforcement with Self-Restraint Training on Reasoning Tasks},
  author={Yu, Zhaoning and Su, Zhaolun and Wang, Haozhu and Weston, Jason and Yu, Ping and Xu, Jing},
  year={2025},
  selected={true},
  preview={restrain.png},
  abstract={Introduced RESTRAIN, a self-penalizing RL framework that transforms unlabeled data into training signals by penalizing overconfident or low-confidence rollouts while preserving promising reasoning chains.}
}

@article{wang2023shepherd,
  title={Shepherd: A Critic for Language Model Generation},
  author={Wang, Tianlu and Yu, Ping and Tan, Ellen and others},
  year={2023},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={shepherd.png},
  abstract={Introduced Shepherd, a 7B-parameter LLM tuned to critique and refine model outputs using a curated feedback dataset. Achieved 53-87% win rates against competitive alternatives.}
}

@article{whitehouse2025j1,
  title={J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning},
  author={Whitehouse, Chenxi and Wang, Tianlu and Yu, Ping and others},
  year={2025},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={j1.png},
  abstract={Introduced J1, a reinforcement learning approach for training LLM-as-a-Judge with verifiable rewards that incentivize reasoning and reduce bias. Outperforms all existing 8B/70B models.}
}

@article{wang2024self,
  title={Self-Taught Evaluators},
  author={Wang, Tianlu and Kulikov, Ilia and Golovneva, Olga and Yu, Ping and others},
  year={2024},
  selected={true},
  pdf={https://arxiv.org/abs/xxxx},
  preview={self_taught.png},
  abstract={Proposed Self-Taught Evaluator, a synthetic-data approach for training LLM-as-a-Judge without human labels. Improved LLaMA3-70B-Instruct from 75.4 to 88.3 on RewardBench.}
}

@article{chameleon2024,
  title={Chameleon: Mixed-Modal Early-Fusion Foundation Models},
  author={Chameleon Team},
  year={2024},
  selected={false},
  pdf={https://arxiv.org/abs/xxxx},
  preview={chameleon.png},
  abstract={Developed Chameleon, a family of early-fusion token-based multimodal models for unified image-text understanding and generation. Achieves state-of-the-art results in image captioning.}
}

@article{opt2022,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning Through the Lens of Generalization},
  author={OPT Team},
  year={2022},
  selected={false},
  pdf={https://arxiv.org/abs/xxxx},
  preview={opt_iml.png},
  abstract={Developed OPT-IML Bench, a large benchmark of 2,000 NLP tasks for studying instruction-tuning decisions. Trained OPT-IML 30B and 175B with improved generalization.}
}